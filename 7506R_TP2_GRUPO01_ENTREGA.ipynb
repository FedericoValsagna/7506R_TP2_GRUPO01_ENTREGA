{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 7506 - Trabajo Práctico 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.metrics import *\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from joblib import load\n",
    "import sklearn as sk"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataset Train preprocesado\n",
    "ds_train = pd.read_csv('datasets/tp1-train_id.csv')\n",
    "ds_train = ds_train.drop(['Unnamed: 0'], axis=1)\n",
    "ds_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Dataset Test preprocesado\n",
    "ds_test = pd.read_csv('datasets/tp1-test_id.csv')\n",
    "ds_test = ds_test.drop(['Unnamed: 0'], axis=1)\n",
    "ds_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Procesamiento del lenguaje natural"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Ampliación del dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Este dataset incluye descripciones de las propiedades del otro dataset. Veremos como podemos extraer información de estas descripciones."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "descriptions_dataset = pd.read_csv('datasets/properati_argentina_2021_decrip.csv')\n",
    "descriptions_dataset.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tomaremos las descripciones correspondientes a los datasets de train y test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "descriptions_train = descriptions_dataset[descriptions_dataset.id.isin(ds_train.id)].copy()\n",
    "descriptions_test = descriptions_dataset[descriptions_dataset.id.isin(ds_test.id)].copy()\n",
    "descriptions_train.shape, descriptions_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Análisis de sentimientos - Tecnica Minqing Hu y Bing Liu"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Una forma de analizar el sentimiento de un de un texto es considerando su sentimiento como la suma de los sentimientos de cada una de las palabras que lo forman.\n",
    "\n",
    "Para el analisis de sentimiento nos guiamos del analisis realizado en esta pagina: https://www.cienciadedatos.net/documentos/py25-text-mining-python.html\n",
    "\n",
    "Utilizamos algunas funciones de tokenizacion y limpieza de ahi con alguna sutil modificacion para nuestro caso de uso en particular."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def limpiar_tokenizar(texto):\n",
    "    '''\n",
    "    Esta función limpia y tokeniza el texto en palabras individuales.\n",
    "    El orden en el que se va limpiando el texto no es arbitrario.\n",
    "    El listado de signos de puntuación se ha obtenido de: print(string.punctuation)\n",
    "    y re.escape(string.punctuation)\n",
    "    '''\n",
    "\n",
    "    # Se convierte todo el texto a minúsculas\n",
    "    nuevo_texto = str(texto).lower()\n",
    "    # Eliminación de páginas web (palabras que empiezan por \"http\")\n",
    "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
    "    # Eliminación de signos de puntuación\n",
    "    regex = '[\\\\!\\\\¡\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
    "    nuevo_texto = re.sub(regex, ' ', nuevo_texto)\n",
    "    # Eliminación de números\n",
    "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
    "    # Eliminación de espacios en blanco múltiples\n",
    "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
    "    # Tokenización por palabras individuales\n",
    "    nuevo_texto = nuevo_texto.split(sep=' ')\n",
    "    # Eliminación de tokens con una longitud < 2\n",
    "    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]\n",
    "\n",
    "    return (nuevo_texto)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# se aplica la función de limpieza a train y test y tokenización a cada descripcion\n",
    "\n",
    "tokenized_train = pd.concat(\n",
    "    [descriptions_train.id, descriptions_train['property_description'].apply(limpiar_tokenizar)], axis=1)\n",
    "tokenized_test = pd.concat([descriptions_test.id, descriptions_test['property_description'].apply(limpiar_tokenizar)],\n",
    "                           axis=1)\n",
    "tokenized_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Separamos los tokens según ids tanto en train como en test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens_train = tokenized_train.explode(column='property_description')\n",
    "tokens_train = tokens_train.rename(columns={'property_description': 'token'})\n",
    "tokens_train.reset_index(inplace=True, drop=True)\n",
    "tokens_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replicamos en test.\n",
    "tokens_test = tokenized_test.explode(column='property_description')\n",
    "tokens_test = tokens_test.rename(columns={'property_description': 'token'})\n",
    "tokens_test.reset_index(inplace=True, drop=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens_train.shape, tokens_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vemos que tenemos 11 millones de palabras en train y 3 millones en test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notamos que las preposiciones no son relevantes para entender que atributo podria ser mejor para expandir el datast, asi que decidimos agregarlas como stopwords.\n",
    "\n",
    "Tampoco van a variar mucho el analisis de sentimiento realizado en este trabajo."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## listado de stopwords\n",
    "\n",
    "preposiciones = [\"a\", \"ante\", \"bajo\", \"cabe\", \"con\", \"contra\", \"de\", \"desde\", \"durante\", \"en\", \"entre\", \"hacia\",\n",
    "                 \"hasta\", \"mediante\", \"para\", \"por\", \"según\", \"sin\", \"sobre\", \"tras\", \"vía\"]\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "stop_words += preposiciones\n",
    "\n",
    "# filtrado para excluir stopwords\n",
    "tokens_train = tokens_train[~(tokens_train[\"token\"].isin(stop_words))]\n",
    "\n",
    "tokens_test = tokens_test[~(tokens_test[\"token\"].isin(stop_words))]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Agregamos un lexicon en español de esta pagina: https://github.com/jboscomendoza/lexicos-nrc-afinn"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# lexicon sentimientos\n",
    "lexicon = pd.read_csv('datasets/lexico_nrc.csv')\n",
    "lexicon"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def mappear_valores_sentimiento(s):\n",
    "    # 1 Positivo\n",
    "    # 0 Neutro\n",
    "    # -1 Negativo\n",
    "    sentimiento_numerico = 0\n",
    "    if str(s) in ['negativo', 'tristeza', 'miedo', 'enfado', 'tristeza', 'asco']:\n",
    "        sentimiento_numerico = -1\n",
    "    if str(s) in ['sorpresa', 'positivo', 'confianza', 'alegría']:\n",
    "        sentimiento_numerico = 1\n",
    "    if str(s) in ['anticipación']:\n",
    "        sentimiento_numerico = 0\n",
    "\n",
    "    return sentimiento_numerico\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lexicon['sentimiento'] = lexicon['sentimiento'].apply(lambda x: mappear_valores_sentimiento(x))\n",
    "lexicon[['sentimiento']].head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokens_train[tokens_train.token.isin(lexicon.palabra)].shape, tokens_test[tokens_test.token.isin(lexicon.palabra)].shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "lexicon"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Tenemos un millon y medio de coincidencias con el lexicón en train. Usaremos estos sentimientos para puntuar las propiedades"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# sentimiento promedio de cada descripcion\n",
    "tokens_sentimientos_train = pd.merge(\n",
    "    left=tokens_train,\n",
    "    right=lexicon,\n",
    "    left_on=\"token\",\n",
    "    right_on=\"palabra\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "tokens_sentimientos_train = tokens_sentimientos_train.drop(columns=[\"palabra\", \"word\"])\n",
    "\n",
    "tokens_sentimientos_test = pd.merge(\n",
    "    left=tokens_test,\n",
    "    right=lexicon,\n",
    "    left_on=\"token\",\n",
    "    right_on=\"palabra\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "tokens_sentimientos_test = tokens_sentimientos_test.drop(columns=[\"palabra\", \"word\"])\n",
    "\n",
    "tokens_sentimientos_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ahora calcularemos el puntaje para cada propiedad como la suma de los sentimientos."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_train = tokens_sentimientos_train[[\"id\", \"token\", \"sentimiento\"]].groupby([\"id\"]).sum().reset_index()\n",
    "\n",
    "score_test = tokens_sentimientos_test[[\"id\", \"token\", \"sentimiento\"]].groupby([\"id\"]).sum().reset_index()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_train.sentimiento.max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "La mejor propiedad tiene un puntaje de 216. Analizaremos un poco las descripciones de las mejores y peores."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top5_positivas = score_train.sort_values(by='sentimiento', ascending=False).head(5)\n",
    "top5_positivas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "descriptions_train.property_description.iloc[top5_positivas.index]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_train.sentimiento.min()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "top5_negativas = score_train.sort_values(by='sentimiento').head(5)\n",
    "top5_negativas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "descriptions_train.iloc[top5_negativas.index].property_description"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "descriptions_test.head()Como es claro, la gente que publica la venta de una propiedad va a tratar de expresar la mejor publicacion y descripcion posible. Es por eso que tenemos una tasa altisima de positividad. No buscamos hacer un analisis tan profundo de las descripciones sino crear un puntaje relativamente estandarizado para poder usar la descripción como feature."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def perfil_sentimientos(title, df):\n",
    "    print(title)\n",
    "    print(f\"Positivos: {round(100 * np.mean(df.sentimiento > 0), 2)}\")\n",
    "    print(f\"Neutros  : {round(100 * np.mean(df.sentimiento == 0), 2)}\")\n",
    "    print(f\"Negativos: {round(100 * np.mean(df.sentimiento < 0), 2)}\")\n",
    "\n",
    "\n",
    "perfil_sentimientos(\"Train: \", score_train)\n",
    "print()\n",
    "perfil_sentimientos(\"Test: \", score_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finalmente, agregaremos nuestro puntaje como columnas nuevas del dataset."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_train = pd.merge(ds_train, score_train, on='id')\n",
    "ds_train.rename(columns={'sentimiento': 'score_sentimientos'}, inplace=True)\n",
    "ds_test = pd.merge(ds_test, score_test, on='id')\n",
    "ds_test.rename(columns={'sentimiento': 'score_sentimientos'}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Tecnica Regex"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Revisamos las siguientes paginas para entender cuales son los ammenities mas buscados en CABA, y en Argentina en general.\n",
    "\n",
    "https://www.iprofesional.com/negocios/371702-cuales-son-los-amenities-mas-exoticos-de-edificios-en-argentina\n",
    "\n",
    "https://www.baenegocios.com/sociedad/Ranking-de-amenities-los-servicios-que-mas-pesan-al-comprar-una-propiedad-20220119-0068.html\n",
    "\n",
    "https://www.forbesargentina.com/negocios/amenities-servicios-mas-demandados-argentinos-comprar-una-propiedad-n11901\n",
    "\n",
    "Dichos ammenities parecen hacer que la propiedad cotice entre un 15% y un 20% más que el precio de venta.\n",
    "\n",
    "Sacando un promedio y haciendo un top-5 ranking, podemos notar que los mas relevantes son:\n",
    "\n",
    "- Garage/Estacionamiento\n",
    "- Pileta\n",
    "- Jardin/Espacio al aire libre\n",
    "- Parrilla\n",
    "- SUM (Gimnasio/Spa/Sauna)\n",
    "\n",
    "Al buscar estos datos, podriamos tratar de entender si el precio resulta mayor, contra una propiedad de similares caracteristicas pero sin estos ammenities y a partir de eso, entender que % varía del precio de venta original."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def calculate_freq(feature, regex):\n",
    "    freq = descriptions_train.property_description.str.contains(regex, regex=True).sum()\n",
    "    print(\n",
    "        f\"Los anuncios de propiedades que tienen la feature {feature} son: {freq} y representan el {freq * 100 // len(descriptions_train)}% de los datos\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Amenities"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "garage = re.compile(r\"\\s*garage|garaje|estacionamiento|parking\")\n",
    "calculate_freq(\"Garage\", garage)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pileta = re.compile(r\"\\s*pileta\")\n",
    "calculate_freq(\"Pileta\", pileta)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "jardin = re.compile(r\"\\s*jardin|espacio verde\")\n",
    "calculate_freq(\"Jardín\", jardin)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "parrilla = re.compile(r\"\\s*parrilla|bbq\")\n",
    "calculate_freq(\"Parrilla\", parrilla)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sum = re.compile(r\"\\s*zoom|sum|gimansio|spa\")\n",
    "calculate_freq(\"SUM\", sum)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "balcon = re.compile(r\"\\s*balcon|balcón\")\n",
    "calculate_freq(\"Balcón\", balcon)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Otro aspecto interesante que dejamos fuera del análisis es a que tipo de vivienda pertenece cada ammenity. Y si donde encontramos una amenity en particular, encontramos consecuentemente otra. Por ejemplo, una casa con jardin y parrila y/o pileta. De esta manera podriamos tratar de determinar el costo de cada ammenity o como afecta al precio."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por último, construiremos columnas booleanas para los mejores features y las agregaremos a nuestros datasets. Usaremos parrilla, sum, balcón y pileta."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "amenities_train = pd.DataFrame({\n",
    "    'id': descriptions_train.id,\n",
    "    'pileta': descriptions_train.property_description.str.contains(pileta, regex=True),\n",
    "    'parrilla': descriptions_train.property_description.str.contains(parrilla, regex=True),\n",
    "    'balcon': descriptions_train.property_description.str.contains(balcon, regex=True),\n",
    "    'sum': descriptions_train.property_description.str.contains(sum, regex=True)\n",
    "})\n",
    "amenities_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Replicamos lo mismo en test\n",
    "amenities_test = pd.DataFrame({\n",
    "    'id': descriptions_test.id,\n",
    "    'pileta': descriptions_test.property_description.str.contains(pileta, regex=True),\n",
    "    'parrilla': descriptions_test.property_description.str.contains(parrilla, regex=True),\n",
    "    'balcon': descriptions_test.property_description.str.contains(balcon, regex=True),\n",
    "    'sum': descriptions_test.property_description.str.contains(sum, regex=True)\n",
    "})\n",
    "amenities_test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_train = pd.merge(ds_train, amenities_train, on='id')\n",
    "ds_train.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_test = pd.merge(ds_test, amenities_test, on='id')\n",
    "ds_test.head()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Expensas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por último, trabajaremos en crear una columna numérica con el valor de las expensas. Evaluaremos primero que porcentaje de valores podemos conseguir con regex"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "expensas = re.compile(r\"\\s*[0-9.]*\\s*exp|expensas\")\n",
    "calculate_freq(\"Expensas\", expensas)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "expensas_extract = re.compile('((?:[a-zA-Z0-9]+\\s*){5}(?:expensas|exp)\\s(?:[a-zA-Z0-9]+\\s){10})')\n",
    "expensas_train = descriptions_train.property_description.str.extract(expensas_extract)\n",
    "expensas_train.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "expensas_extract = re.compile('((?:[0-9a-zA-Z,.]+\\s*){5}?(?:con|sin|de)\\s*(?:expensas|exp))')\n",
    "expensas_train = descriptions_train.sample(100).property_description.str.extract(expensas_extract)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sin_expensas = re.compile('\\s*([0-9.]+)\\s*exp|expensas')\n",
    "descriptions_train.property_description.str.extract(sin_expensas)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Modelos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Selección de features"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Para entrenar los modelos usaremos nuestro dataset recien generado, descartaremos el id, el título y las fechas. Convertiremos las categóricas en variables numéricas."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_train.columns"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_trabajo_train = ds_train.drop(['id', 'property_title', 'start_date', 'end_date'], axis=1)\n",
    "ds_trabajo_train['place_l3'] = pd.factorize(ds_train['place_l3'])[0]\n",
    "ds_trabajo_train['property_type'] = pd.factorize(ds_train['property_type'])[0]\n",
    "ds_trabajo_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "## Replicamos en test\n",
    "ds_trabajo_test = ds_test.drop(['id', 'property_title', 'start_date', 'end_date'], axis=1)\n",
    "ds_trabajo_test['place_l3'] = pd.factorize(ds_test['place_l3'])[0]\n",
    "ds_trabajo_test['property_type'] = pd.factorize(ds_test['property_type'])[0]\n",
    "ds_trabajo_test"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "ds_trabajo_train.shape, ds_trabajo_test.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Sacamos la variable target y creamos nuestros datasets de entrenamiento"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "columnas_predictoras = ds_trabajo_train.columns.to_list()\n",
    "columnas_predictoras.remove('property_price')\n",
    "columnas_predictoras"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train_tp1 = ds_trabajo_train.loc[:, ['latitud',\n",
    "                                       'longitud',\n",
    "                                       'place_l3',\n",
    "                                       'property_type',\n",
    "                                       'property_rooms',\n",
    "                                       'property_bedrooms',\n",
    "                                       'property_surface_total',\n",
    "                                       'property_surface_covered']]\n",
    "x_train = ds_trabajo_train.loc[:, columnas_predictoras]\n",
    "\n",
    "x_test_tp1 = ds_trabajo_test.loc[:, ['latitud',\n",
    "                                     'longitud',\n",
    "                                     'place_l3',\n",
    "                                     'property_type',\n",
    "                                     'property_rooms',\n",
    "                                     'property_bedrooms',\n",
    "                                     'property_surface_total',\n",
    "                                     'property_surface_covered']]\n",
    "x_test = ds_trabajo_test.loc[:, columnas_predictoras]\n",
    "\n",
    "y_train = ds_trabajo_train.property_price\n",
    "y_test = ds_trabajo_test.property_price"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exportamos los dataset de train y test:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([x_train, y_train], axis=1).to_csv('datasets/train.csv')\n",
    "pd.concat([x_test, y_test], axis=1).to_csv('datasets/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como todas las features que tenemos están en escalas completamente diferentes y no pueden compararse, normalizaremos el dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sscaler = StandardScaler()\n",
    "x_train_tp1_transform = sscaler.fit_transform(pd.DataFrame(x_train_tp1))\n",
    "x_test_tp1_transform = sscaler.fit_transform(pd.DataFrame(x_test_tp1))\n",
    "x_train_transform = sscaler.fit_transform(pd.DataFrame(x_train))\n",
    "x_test_transform = sscaler.fit_transform(pd.DataFrame(x_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### XGBoost - Regresión"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "best_xgb_tp1 es el arbol que tiene los mejores hiper parametros y estimaodores obtenidos en el TP1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def regression_metrics(title, real, predicted):\n",
    "    mse = sk.metrics.mean_squared_error(y_true=real, y_pred=predicted)\n",
    "    rmse = sk.metrics.mean_squared_error(y_true=real, y_pred=predicted, squared=False)\n",
    "    r2 = sk.metrics.r2_score(y_true=real, y_pred=predicted)\n",
    "\n",
    "    print(title)\n",
    "    print(f\"El error (mse) es: {mse}\")\n",
    "    print(f\"El error (rmse) es: {rmse}\")\n",
    "    print(f\"El error (r²) es: {r2}\\n\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_xgb_tp1 = load('XGBoost-pca.joblib')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_xgb_tp1.fit(x_train_transform, y_train)\n",
    "y_pred = best_xgb_tp1.predict(x_train_transform)\n",
    "y_pred_test = best_xgb_tp1.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Metricas obtenidas con el dataset del TP1."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Metricas XGBoost optimizado - Train\n",
    "\n",
    "El error (mse) es: 761704416.3937123\n",
    "\n",
    "El error (rmse) es: 27598.993032241455\n",
    "\n",
    "El error (r²) es: 0.9652453545965177\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "Metricas XGBoost optimizado - Test\n",
    "\n",
    "El error (mse) es: 3287902738.8784523\n",
    "\n",
    "El error (rmse) es: 57340.23664825994\n",
    "\n",
    "El error (r²) es: 0.8477123168506954"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regression_metrics(\"Metricas XGBoost optimizado - Train\", y_train, y_pred)\n",
    "regression_metrics(\"Metricas XGBoost optimizado - Test\", y_test, y_pred_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Utilizamos esta pagina como soporte para entender mejor que representa cada metrica de error: \n",
    "\n",
    "https://sitiobigdata.com/2018/08/27/machine-learning-metricas-regresion-mse/"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Train**\n",
    "\n",
    "Notamos como el modelo con el dataset expandido mejora significativamente.\n",
    "\n",
    "Podemos observar un delta de **MSE** de: 432564299. Esto significa que el error se redujo un 400%. Esto es relevante ya que esta metrica nos dice sobre cuan bueno es realmente el modelo entrenado.\n",
    "\n",
    "Luego, para **RMSE** tenemos un delta de: 9456. Esto significa que el error se redujo un 65%.\n",
    "\n",
    "Finalmente para **R2** obtuvimos un delta de: -0,19. Esto significa que el modelo mejoró un 20%, ya que, mientras mas tengamos un valor mas cercano a uno, tenemos un modelo con un error cercano a cero."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Test**\n",
    "\n",
    "Notamos como el modelo con el dataset expandido mejora significativamente.\n",
    "\n",
    "Podemos observar un delta de **MSE** de: -96136560337. Esto significa que el error en test empeoró un 3%. \n",
    "\n",
    "Luego, para **RMSE** tenemos un delta de: -257976,21245748526. Esto significa que el error empeoró un 550%.\n",
    "\n",
    "Finalmente para **R2** obtuvimos un delta de: 0,52. Esto significa que el modelo empeoró un 52%."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Nuevos Hiperparametros optimizados con el nuevo dataset ampliado."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "#Cantidad de combinaciones que quiero porbar\n",
    "n = 10\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": [3, 12, 4],\n",
    "    \"learning_rate\": [0.02, 0.03, 0.06],\n",
    "    \"min_child_weight\": [2, 12, 2],\n",
    "    \"n_estimators\": [100, 350],\n",
    "    'alpha': np.linspace(0.03, 0.09, n),\n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "search_regressor = XGBRegressor()\n",
    "\n",
    "search = RandomizedSearchCV(search_regressor, params, cv=5, random_state=9, n_iter=10, verbose=10000)\n",
    "\n",
    "search.fit(x_train_transform, y_train)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mejores Hiperparámetros\n",
    "search.best_params_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Mejor Metrica\n",
    "search.best_score_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "best_xgb = search.best_estimator_\n",
    "best_xgb.fit(x_train_transform, y_train)\n",
    "y_pred = best_xgb.predict(x_train_transform)\n",
    "y_pred_test = best_xgb.predict(x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regression_metrics(\"Metricas XGBoost optimizado - Train\", y_train, y_pred)\n",
    "regression_metrics(\"Metricas XGBoost optimizado - Test\", y_test, y_pred_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Train**\n",
    "\n",
    "Notamos como el modelo con el dataset sigue teniendo muy buenos resultados para el dataset de Train pero con una leve baja.\n",
    "\n",
    "Con respecto a las metricas obtenidas con el dataset del TP1, podemos observar un delta de **MSE** de: 617388041. Esto significa que el error, en comparacion con las metricas anteriores se incrementó un 287%.\n",
    "\n",
    "Luego, para **RMSE** tenemos un delta de: 12623. Esto significa que el error se redujo un 3%.\n",
    "\n",
    "Finalmente para **R2** obtuvimos un delta de: -0,02. Esto significa que el modelo empeoró un 2%."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Test**\n",
    "\n",
    "Notamos como el modelo con el dataset expandido y la busqueda de un nuevo arbol con mejores hiper-parametros, resulto en un pequeño detrimento del dataset de train pero mejoró mucho mas el dataset de test.\n",
    "\n",
    "Con respecto a las metricas obtenidas con el dataset del TP1, podemos observar un delta de **MSE** de: 45032742345. Esto significa que el error en test mejoró un 54%. \n",
    "\n",
    "Luego, para **RMSE** tenemos un delta de: 82096. Esto significa que el error mejoró un 73%.\n",
    "\n",
    "Finalmente para **R2** obtuvimos un delta de: -9795260413860356. Esto significa que el modelo mejoró un 405%."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "Como **conclusion**, notamos que el dataset expandido nos mejora el modelo con el dataset de train un 20% pero para el dataset de test, nos lo empeora un 52%.\n",
    "\n",
    "Cuando hicimos la busqueda de mejores hiper-parametros para el nuevo dataset expandido, logramos balancear estos resultados.\n",
    "\n",
    "Finalmente, haciendo el delta final, el resultado de expandir el dataset nos resulto en una mejora del 18% para el dataset de train y en un 353% de mejora en el dataset de test."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Redes Neuronales"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Regresión"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usaremos el dataset del tp1 normalizado"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train_tp1_transform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Predecir el precio de la propiedad y utilizar como métrica de evaluación el error cuadrático medio."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vamos a predecir el precio de la propiedad (dolares) en base a la superifice total y cubierta. Ya que como vimos en el trabajo pasado, eran los atributos que mas se correlacionaban con el precio.\n",
    "\n",
    "(Todas columnas tienen que ser numericas para Redes Neuronales)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def regression_scatter(x, y_true, y_pred):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Precio según {x.name}\")\n",
    "    sns.scatterplot(x=x, y=y_true, ax=ax1)\n",
    "    ax1.set_title(f\"{x.name} vs Precio real\")\n",
    "\n",
    "    sns.scatterplot(x=x, y=y_pred, ax=ax2)\n",
    "    ax2.set_title(f\"{x.name} vs Precio predicho\")\n",
    "\n",
    "    sns.scatterplot(x=x, y=y_true, ax=ax3)\n",
    "    sns.scatterplot(x=x, y=y_pred, ax=ax3)\n",
    "    ax3.set_title(f\"Grafico combinado\");"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Construcción del modelo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.legend()\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Error [MSE]')\n",
    "    plt.grid(True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Modelo base"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "d_in = x_train_tp1_transform.shape[1]\n",
    "d_out = 1\n",
    "\n",
    "\n",
    "def base_model_builder():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(d_in, input_shape=(d_in,), kernel_initializer='normal', activation='relu'),\n",
    "        keras.layers.Dense(d_out, kernel_initializer='normal', activation='relu')])\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    print(model.summary())\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_model = base_model_builder()\n",
    "base_history = base_model.fit(\n",
    "    x_train_tp1_transform,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=1000,\n",
    "    validation_split=0.33,\n",
    "    verbose=0,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_loss(base_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Evaluamos la predicción inicial para train"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_prices_dist(y_pred, y_pred_test):\n",
    "    precios_train = pd.concat([y_train, pd.DataFrame(y_pred, columns=['predicted'])], axis=1)\n",
    "    precios_test = pd.concat([y_test, pd.DataFrame(y_pred_test, columns=['predicted'])], axis=1)\n",
    "    fig, axs = plt.subplots(1, 3, figsize=(25, 5))\n",
    "\n",
    "    fig.suptitle(\"Distribución de precios reales y predichos\")\n",
    "\n",
    "    axs[0].set_xlabel(\"Precio\")\n",
    "    axs[0].set_ylabel(\"Densidad\")\n",
    "    axs[0].set_title(\"Train\")\n",
    "    sns.kdeplot(precios_train['property_price'], ax=axs[0])\n",
    "    sns.kdeplot(precios_train['predicted'], ax=axs[0])\n",
    "    axs[0].legend(labels=['Real', 'Predicho'])\n",
    "\n",
    "    axs[1].set_xlabel(\"Precio\")\n",
    "    axs[1].set_ylabel(\"Densidad\")\n",
    "    axs[1].set_title(\"Test\")\n",
    "    sns.kdeplot(precios_test['property_price'], ax=axs[1])\n",
    "    sns.kdeplot(precios_test['predicted'], ax=axs[1])\n",
    "    axs[1].legend(labels=['Real', 'Predicho'])\n",
    "\n",
    "    axs[2].set_xlabel(\"Precio Real\")\n",
    "    axs[2].set_ylabel(\"Precio Predicho\")\n",
    "    axs[2].set_title(\"Real vs predicho\")\n",
    "    sns.scatterplot(x=precios_train['property_price'], y=precios_train['predicted'], ax=axs[2])\n",
    "    sns.regplot(x=precios_train['property_price'], y=precios_train['predicted'], scatter=False, ax=axs[2], fit_reg=True, color='darkgreen', ci=0)\n",
    "    axs[2].legend(labels=['Real', 'Predicho'])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_base = base_model.predict(x_train_tp1_transform)\n",
    "y_pred_test_base = base_model.predict(x_test_tp1_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prices_dist(y_pred_base, y_pred_test_base)\n",
    "regression_metrics(\"Metricas Red Neuronal Base - Train\", y_train, y_pred_base)\n",
    "regression_metrics(\"Metricas Red Neuronal Base - Test\", y_test, y_pred_test_base)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vemos que hay mucha dispersión respecto a los precios reales, probaremos un modelo profundo con una capa intermedia con la mitad de las neuronas de la primera."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Modelo de 3 capas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def larger_model_builder():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(d_in, input_shape=(d_in,), kernel_initializer='normal', activation='relu'),\n",
    "        keras.layers.Dense(int(d_in / 2), kernel_initializer='normal', activation='relu'),\n",
    "        keras.layers.Dense(1, kernel_initializer='normal', activation='relu')\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "larger_model_builder().summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "larger_model = larger_model_builder()\n",
    "larger_history = larger_model.fit(\n",
    "    x_train_tp1_transform,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=1000,\n",
    "    validation_split=0.33,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_loss(larger_history)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_larger = larger_model.predict(x_train_tp1_transform)\n",
    "y_pred_larger_test = larger_model.predict(x_test_tp1_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prices_dist(y_pred_larger, y_pred_larger_test)\n",
    "regression_metrics(\"Metricas Red Neuronal Profunda - Train\", y_train, y_pred_larger)\n",
    "regression_metrics(\"Metricas Red Neuronal Profunda - Test\", y_test, y_pred_larger_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Por último, probaremos un tercer modelo con una capa inical más ancha."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Modelo ancho"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def wider_model_builder():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(d_in * 2, input_shape=(d_in,), kernel_initializer='normal', activation='relu'),\n",
    "        keras.layers.Dense(1, kernel_initializer='normal', activation='relu')\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wider_model_builder().summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "wider_model = wider_model_builder()\n",
    "wider_history = wider_model.fit(\n",
    "    x_train_tp1_transform,\n",
    "    y_train,\n",
    "    epochs=1000,\n",
    "    batch_size=1000,\n",
    "    validation_split=0.33,\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "y_pred_wider = wider_model.predict(x_train_tp1_transform)\n",
    "y_pred_wider_test = wider_model.predict(x_test_tp1_transform)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prices_dist(y_pred_wider, y_pred_wider_test)\n",
    "regression_metrics(\"Metricas Red Neuronal Profunda - Train\", y_train, y_pred_wider)\n",
    "regression_metrics(\"Metricas Red Neuronal Profunda - Test\", y_test, y_pred_wider_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Análisis de métricas"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_prices_dist(y_pred_larger, y_pred_larger_test)\n",
    "regression_metrics(\"Metricas Red Neuronal Profunda - Train\", y_train, y_pred_larger)\n",
    "regression_metrics(\"Metricas Red Neuronal Profunda - Test\", y_test, y_pred_larger_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "regression_scatter(ds_train.property_surface_covered, y_train, y_pred_larger[:,0])\n",
    "regression_scatter(ds_train.property_surface_total, y_train, y_pred_larger[:,0])\n",
    "regression_scatter(ds_train.latitud, y_train,y_pred_larger[:,0])\n",
    "regression_scatter(ds_train.longitud, y_train,y_pred_larger[:,0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Podemos ver que las predicciones mejoraron muchísimo y que los precios están cerca de los reales. Sin embargo, falta bastante para llegar a un buen resultado."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### Cosas a probar para mejorar el modelo:\n",
    "- Más capas intermedias\n",
    "- Diferentes funciones de activación\n",
    "- Usar otro escalado"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Clasificación"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def predicciones_clasificacion(modelo, x_train, x_test):\n",
    "    # Predicciones Train\n",
    "    output_modelo = modelo.predict(x_train)\n",
    "\n",
    "    predicciones = np.argmax(output_modelo, axis=1).tolist()\n",
    "    valores_esperados = np.argmax(y_train_encoded, axis=1).tolist()\n",
    "    matriz_de_metricas = confusion_matrix(predicciones, valores_esperados)\n",
    "\n",
    "    sns.heatmap(matriz_de_metricas,annot=True, cmap = 'Blues', fmt= 'g').set(title='Predicciones sobre el conjunto de entrenamiento')\n",
    "    plt.xlabel('Valores predichos')\n",
    "    plt.ylabel('Valores reales')\n",
    "    plt.show()\n",
    "    print(classification_report(predicciones, valores_esperados))\n",
    "\n",
    "\n",
    "    # Predicciones Test\n",
    "    output_modelo = modelo.predict(x_test)\n",
    "\n",
    "    predicciones = np.argmax(output_modelo, axis=1).tolist()\n",
    "    valores_esperados = np.argmax(y_test_encoded, axis=1).tolist()\n",
    "    matriz_de_metricas = confusion_matrix(predicciones, valores_esperados)\n",
    "\n",
    "    sns.heatmap(matriz_de_metricas,annot=True, cmap = 'Blues', fmt= 'g').set(title='Predicciones sobre el conjunto de testeo')\n",
    "    plt.xlabel('Valores predichos')\n",
    "    plt.ylabel('Valores reales')\n",
    "    plt.show()\n",
    "    print(classification_report(predicciones, valores_esperados))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Construcción del target"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "ohe = OneHotEncoder()\n",
    "\n",
    "ds_train_ohe= ds_train.copy()\n",
    "property_type_encoded = ohe.fit_transform(ds_train_ohe[['property_type']].astype(str)).todense().astype(int)\n",
    "property_type_encoded = pd.DataFrame(property_type_encoded).add_prefix('property_type_')\n",
    "\n",
    "ds_train_encoded = pd.get_dummies(ds_train_ohe, columns=['property_type'])\n",
    "# Droppeamos todas las columnas del dataset que no vamos a utilizar\n",
    "x_train = ds_train_encoded.drop(axis = 1, columns = [\n",
    "                                             \"id\",\n",
    "                                             \"start_date\",\n",
    "                                             \"end_date\",\n",
    "                                             \"place_l3\",\n",
    "                                             \"property_rooms\",\n",
    "                                             \"property_bedrooms\",\n",
    "                                             \"property_price\",\n",
    "                                             \"property_title\",\n",
    "                                             \"pxm2\",\n",
    "                                             'tipo_precio',\n",
    "                                                    ])\n",
    "\n",
    "# Escalamos los datos\n",
    "x_train = scaler.fit_transform(pd.DataFrame(x_train))\n",
    "\n",
    "# Construimos el target con la variable objetivo\n",
    "y_train = ds_train.tipo_precio\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "# Repetimos los pasos con el dataset de test\n",
    "ds_test_ohe= ds_test.copy()\n",
    "property_type_encoded = ohe.fit_transform(ds_test_ohe[['property_type']].astype(str)).todense().astype(int)\n",
    "property_type_encoded = pd.DataFrame(property_type_encoded).add_prefix('property_type_')\n",
    "ds_test_encoded = pd.get_dummies(ds_test_ohe, columns=['property_type'])\n",
    "x_test = ds_test_encoded.drop(axis=1, columns= [\n",
    "                    'id',\n",
    "                    'start_date',\n",
    "                    'end_date',\n",
    "                    'place_l3',\n",
    "                    'property_rooms',\n",
    "                    'property_bedrooms',\n",
    "                    'property_title',\n",
    "                    'property_price',\n",
    "                    'pxm2',\n",
    "                    'tipo_precio',\n",
    "                             ])\n",
    "\n",
    "x_test = scaler.fit_transform(pd.DataFrame(x_test))\n",
    "y_test = ds_test.tipo_precio\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "\n",
    "# Realizamos el one hot encoder para transformar la variable target en numérica tanto en train como test\n",
    "enc = OneHotEncoder()\n",
    "y_train_encoded = enc.fit_transform(y_train[:, np.newaxis]).toarray()\n",
    "y_test_encoded = enc.transform(y_test[:, np.newaxis]).toarray()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Modelo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cantidad_de_posibles_respuestas=len(np.unique(y_train))\n",
    "cantidad_de_variables_predictoras=len(x_train[0])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Probamos con un modelo base"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo_base = keras.Sequential([\n",
    "        keras.layers.Dense(cantidad_de_posibles_respuestas, input_shape=(cantidad_de_variables_predictoras,), activation= 'softmax')])\n",
    "\n",
    "modelo_base.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo_base.compile(\n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    # Elegimos la siguiente función ya que se trata de una red neuronal de clasificación\n",
    "  loss='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "cant_epochs= 100\n",
    "\n",
    "modelo_base.fit(x_train,y_train_encoded,epochs=cant_epochs,batch_size=16,verbose=True, workers= -1, use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicciones_clasificacion(modelo_base, x_train, x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Probamos ahora agregando una capa intermeda"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo_capa_extra = keras.Sequential([\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras, input_shape=(cantidad_de_variables_predictoras,), activation= 'relu'),\n",
    "        keras.layers.Dense(cantidad_de_posibles_respuestas, activation= 'softmax')\n",
    "])\n",
    "\n",
    "modelo_capa_extra.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo_capa_extra.compile(\n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "  loss='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "cant_epochs= 100\n",
    "\n",
    "modelo_capa_extra.fit(x_train,y_train_encoded,epochs=cant_epochs,batch_size=16,verbose=True, workers= -1, use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicciones_clasificacion(modelo_capa_extra, x_train, x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Optamos por agregar una capa intermedia de 12 neuronas con la función de activación reLU. Luego una capa de 6 neuronas con función tanh. Finalmente una capa de salida de 3 neuronas con la función de activación sigmoidea ya que se trata de un problema de clasificación."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo1 = keras.Sequential([\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras,input_shape=(cantidad_de_variables_predictoras,), activation='relu'),\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras / 2,activation= 'tanh'),\n",
    "        keras.layers.Dense(cantidad_de_posibles_respuestas,activation='softmax')\n",
    "])\n",
    "\n",
    "modelo1.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo1.compile(\n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "  loss='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "cant_epochs= 100\n",
    "\n",
    "modelo1.fit(x_train,y_train_encoded,epochs=cant_epochs,batch_size=16,verbose=True, workers= -1, use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como optimizador decidimos utilizar Adam con un learning rate lo suficientemente bajo como para no realizar saltos demasiado grandes a la hora de converger."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicciones_clasificacion(modelo1, x_train, x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Siguiente modelo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo2 = keras.Sequential([\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras,input_shape=(cantidad_de_variables_predictoras,), activation='relu'),\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras * 2,activation= 'tanh'),\n",
    "        keras.layers.Dense(cantidad_de_posibles_respuestas,activation='softmax')\n",
    "])\n",
    "\n",
    "modelo2.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo2.compile(\n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "  loss='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "cant_epochs= 100\n",
    "\n",
    "historia_entrenamiento_modelo = modelo2.fit(x_train,y_train_encoded,epochs=cant_epochs,batch_size=16,verbose=True, workers= -1, use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicciones_clasificacion(modelo2, x_train, x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vemos que ampliar la cantidad de neuronas de la capa intermedia mejora las métricas del modelo"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo3 = keras.Sequential([\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras,input_shape=(cantidad_de_variables_predictoras,), activation='relu'),\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras * 4,activation= 'tanh'),\n",
    "        keras.layers.Dense(cantidad_de_posibles_respuestas,activation='softmax')\n",
    "])\n",
    "\n",
    "modelo3.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo3.compile(\n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "  loss='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "cant_epochs= 100\n",
    "\n",
    "modelo3.fit(x_train,y_train_encoded,epochs=cant_epochs,batch_size=16,verbose=True, workers= -1, use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicciones_clasificacion(modelo3, x_train, x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "No parecerían mejorar las métricas de test, por lo que seguir agregando neuronas a esta capa resultaría en un overfitteo. Veamos de todos modos que pasa si aumentamos la cantidad de la primera capa."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo4 = keras.Sequential([\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras * 2,input_shape=(cantidad_de_variables_predictoras,), activation='relu'),\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras * 2,activation= 'tanh'),\n",
    "        keras.layers.Dense(cantidad_de_posibles_respuestas,activation='softmax')\n",
    "])\n",
    "\n",
    "modelo4.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo4.compile(\n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "  loss='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "cant_epochs= 100\n",
    "\n",
    "historia_modelo = modelo4.fit(x_train,y_train_encoded,epochs=cant_epochs,batch_size=16,verbose=True, workers= -1, use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicciones_clasificacion(modelo4, x_train, x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Las métricas no mejoraron, nos quedamos con el modelo anterior. Veamos de todos modos si dicho modelo da mejores resultados con su ultima capa siendo de activación Sigmoid en vez de Softmax."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo3_sigmoid = keras.Sequential([\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras,input_shape=(cantidad_de_variables_predictoras,), activation='relu'),\n",
    "        keras.layers.Dense(cantidad_de_variables_predictoras * 2,activation= 'tanh'),\n",
    "        keras.layers.Dense(cantidad_de_posibles_respuestas,activation='sigmoid')\n",
    "])\n",
    "\n",
    "modelo3_sigmoid.summary()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "modelo3_sigmoid.compile(\n",
    "  optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "  loss='categorical_crossentropy',\n",
    ")\n",
    "\n",
    "cant_epochs= 100\n",
    "\n",
    "modelo3_sigmoid.fit(x_train,y_train_encoded,epochs=cant_epochs,batch_size=16,verbose=True, workers= -1, use_multiprocessing=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicciones_clasificacion(modelo3_sigmoid, x_train, x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Vemos que softmax parece dar mejores resultados. Nos quedamos con el anterior modelo como el mejor."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "mejor_modelo = modelo2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Veamos si el modelo está convergiendo bien al mínimo o si está rebotando en puntos debido a un learning rate demasiado alto"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "epochs = range(cant_epochs)\n",
    "plt.plot(epochs, historia_entrenamiento_modelo.history['loss'], color='orange', label='loss')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.title(\"Evolución del score de Loss con los Epochs\")\n",
    "plt.legend()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parecería estar llegando al mínimo correctamente"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Metricas finales"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "predicciones_clasificacion(mejor_modelo, x_train, x_test)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Otro factor interesante que podriamos analizar es a que tipo de vivienda pertenece cada ammenity. Y si donde encontramos una ammenity en particular, encontramos consecuentemente otra. Por ejemplo, una casa con jardin y parrila y/o pileta.\n",
    "De esta manera podriamos tratar de determinar el costo de cada ammenity o como afecta al precio."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Ensambles de modelos"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}