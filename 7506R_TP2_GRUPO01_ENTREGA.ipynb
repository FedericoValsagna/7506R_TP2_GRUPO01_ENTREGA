{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 7506 - Trabajo Práctico 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Introducción"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Librerías"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import re\n",
    "from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, RandomizedSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scikeras.wrappers import KerasClassifier, KerasRegressor\n",
    "from sklearn.metrics import *\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import keras_tuner as kt\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import style\n",
    "from joblib import load\n",
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Train preprocesado\n",
    "ds_train = pd.read_csv('datasets/tp1-train_id.csv')\n",
    "ds_train = ds_train.drop(['Unnamed: 0'], axis=1)\n",
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Dataset Test preprocesado\n",
    "ds_test = pd.read_csv('datasets/tp1-test_id.csv')\n",
    "ds_test = ds_test.drop(['Unnamed: 0'], axis=1)\n",
    "ds_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Procesamiento del lenguaje natural"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ampliación del dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Este dataset incluye descripciones de las propiedades del otro dataset. Veremos como podemos extraer información de estas descripciones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "descriptions_dataset = pd.read_csv('datasets/properati_argentina_2021_decrip.csv')\n",
    "descriptions_dataset.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tomaremos las descripciones correspondientes a los datasets de train y test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "descriptions_train = descriptions_dataset[descriptions_dataset.id.isin(ds_train.id)].copy()\n",
    "descriptions_test = descriptions_dataset[descriptions_dataset.id.isin(ds_test.id)].copy()\n",
    "descriptions_train.shape, descriptions_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Análisis de sentimientos - Tecnica Minqing Hu y Bing Liu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Una forma de analizar el sentimiento de un de un texto es considerando su sentimiento como la suma de los sentimientos de cada una de las palabras que lo forman.\n",
    "\n",
    "Para el analisis de sentimiento nos guiamos del analisis realizado en esta pagina: https://www.cienciadedatos.net/documentos/py25-text-mining-python.html\n",
    "\n",
    "Utilizamos algunas funciones de tokenizacion y limpieza de ahi con alguna sutil modificacion para nuestro caso de uso en particular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def limpiar_tokenizar(texto):\n",
    "    '''\n",
    "    Esta función limpia y tokeniza el texto en palabras individuales.\n",
    "    El orden en el que se va limpiando el texto no es arbitrario.\n",
    "    El listado de signos de puntuación se ha obtenido de: print(string.punctuation)\n",
    "    y re.escape(string.punctuation)\n",
    "    '''\n",
    "\n",
    "    # Se convierte todo el texto a minúsculas\n",
    "    nuevo_texto = str(texto).lower()\n",
    "    # Eliminación de páginas web (palabras que empiezan por \"http\")\n",
    "    nuevo_texto = re.sub('http\\S+', ' ', nuevo_texto)\n",
    "    # Eliminación de signos de puntuación\n",
    "    regex = '[\\\\!\\\\¡\\\\\"\\\\#\\\\$\\\\%\\\\&\\\\\\'\\\\(\\\\)\\\\*\\\\+\\\\,\\\\-\\\\.\\\\/\\\\:\\\\;\\\\<\\\\=\\\\>\\\\?\\\\@\\\\[\\\\\\\\\\\\]\\\\^_\\\\`\\\\{\\\\|\\\\}\\\\~]'\n",
    "    nuevo_texto = re.sub(regex, ' ', nuevo_texto)\n",
    "    # Eliminación de números\n",
    "    nuevo_texto = re.sub(\"\\d+\", ' ', nuevo_texto)\n",
    "    # Eliminación de espacios en blanco múltiples\n",
    "    nuevo_texto = re.sub(\"\\\\s+\", ' ', nuevo_texto)\n",
    "    # Tokenización por palabras individuales\n",
    "    nuevo_texto = nuevo_texto.split(sep=' ')\n",
    "    # Eliminación de tokens con una longitud < 2\n",
    "    nuevo_texto = [token for token in nuevo_texto if len(token) > 1]\n",
    "\n",
    "    return (nuevo_texto)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# se aplica la función de limpieza a train y test y tokenización a cada descripcion\n",
    "\n",
    "tokenized_train = pd.concat(\n",
    "    [descriptions_train.id, descriptions_train['property_description'].apply(limpiar_tokenizar)], axis=1)\n",
    "tokenized_test = pd.concat([descriptions_test.id, descriptions_test['property_description'].apply(limpiar_tokenizar)],\n",
    "                           axis=1)\n",
    "tokenized_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Separamos los tokens según ids tanto en train como en test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens_train = tokenized_train.explode(column='property_description')\n",
    "tokens_train = tokens_train.rename(columns={'property_description': 'token'})\n",
    "tokens_train.reset_index(inplace=True, drop=True)\n",
    "tokens_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Replicamos en test.\n",
    "tokens_test = tokenized_test.explode(column='property_description')\n",
    "tokens_test = tokens_test.rename(columns={'property_description': 'token'})\n",
    "tokens_test.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens_train.shape, tokens_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Vemos que tenemos 11 millones de palabras en train y 3 millones en test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Notamos que las preposiciones no son relevantes para entender que atributo podria ser mejor para expandir el datast, asi que decidimos agregarlas como stopwords.\n",
    "\n",
    "Tampoco van a variar mucho el analisis de sentimiento realizado en este trabajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## listado de stopwords\n",
    "\n",
    "preposiciones = [\"a\", \"ante\", \"bajo\", \"cabe\", \"con\", \"contra\", \"de\", \"desde\", \"durante\", \"en\", \"entre\", \"hacia\",\n",
    "                 \"hasta\", \"mediante\", \"para\", \"por\", \"según\", \"sin\", \"sobre\", \"tras\", \"vía\"]\n",
    "\n",
    "stop_words = []\n",
    "\n",
    "stop_words += preposiciones\n",
    "\n",
    "# filtrado para excluir stopwords\n",
    "tokens_train = tokens_train[~(tokens_train[\"token\"].isin(stop_words))]\n",
    "\n",
    "tokens_test = tokens_test[~(tokens_test[\"token\"].isin(stop_words))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Agregamos un lexicon en español de esta pagina: https://github.com/jboscomendoza/lexicos-nrc-afinn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# lexicon sentimientos\n",
    "lexicon = pd.read_csv('datasets/lexico_nrc.csv')\n",
    "lexicon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def mappear_valores_sentimiento(s):\n",
    "    # 1 Positivo\n",
    "    # 0 Neutro\n",
    "    # -1 Negativo\n",
    "    sentimiento_numerico = 0\n",
    "    if str(s) in ['negativo', 'tristeza', 'miedo', 'enfado', 'tristeza', 'asco']:\n",
    "        sentimiento_numerico = -1\n",
    "    if str(s) in ['sorpresa', 'positivo', 'confianza', 'alegría']:\n",
    "        sentimiento_numerico = 1\n",
    "    if str(s) in ['anticipación']:\n",
    "        sentimiento_numerico = 0\n",
    "\n",
    "    return sentimiento_numerico\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lexicon['sentimiento'] = lexicon['sentimiento'].apply(lambda x: mappear_valores_sentimiento(x))\n",
    "lexicon[['sentimiento']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "tokens_train[tokens_train.token.isin(lexicon.palabra)].shape, tokens_test[tokens_test.token.isin(lexicon.palabra)].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "lexicon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Tenemos un millon y medio de coincidencias con el lexicón en train. Usaremos estos sentimientos para puntuar las propiedades"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# sentimiento promedio de cada descripcion\n",
    "tokens_sentimientos_train = pd.merge(\n",
    "    left=tokens_train,\n",
    "    right=lexicon,\n",
    "    left_on=\"token\",\n",
    "    right_on=\"palabra\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "tokens_sentimientos_train = tokens_sentimientos_train.drop(columns=[\"palabra\", \"word\"])\n",
    "\n",
    "tokens_sentimientos_test = pd.merge(\n",
    "    left=tokens_test,\n",
    "    right=lexicon,\n",
    "    left_on=\"token\",\n",
    "    right_on=\"palabra\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "tokens_sentimientos_test = tokens_sentimientos_test.drop(columns=[\"palabra\", \"word\"])\n",
    "\n",
    "tokens_sentimientos_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ahora calcularemos el puntaje para cada propiedad como la suma de los sentimientos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score_train = tokens_sentimientos_train[[\"id\", \"token\", \"sentimiento\"]].groupby([\"id\"]).sum().reset_index()\n",
    "\n",
    "score_test = tokens_sentimientos_test[[\"id\", \"token\", \"sentimiento\"]].groupby([\"id\"]).sum().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score_train.sentimiento.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "La mejor propiedad tiene un puntaje de 216. Analizaremos un poco las descripciones de las mejores y peores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top5_positivas = score_train.sort_values(by='sentimiento', ascending=False).head(5)\n",
    "top5_positivas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "descriptions_train.property_description.iloc[top5_positivas.index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "score_train.sentimiento.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "top5_negativas = score_train.sort_values(by='sentimiento').head(5)\n",
    "top5_negativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "descriptions_train.iloc[top5_negativas.index].property_description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "descriptions_test.head()Como es claro, la gente que publica la venta de una propiedad va a tratar de expresar la mejor publicacion y descripcion posible. Es por eso que tenemos una tasa altisima de positividad. No buscamos hacer un analisis tan profundo de las descripciones sino crear un puntaje relativamente estandarizado para poder usar la descripción como feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def perfil_sentimientos(title, df):\n",
    "    print(title)\n",
    "    print(f\"Positivos: {round(100 * np.mean(df.sentimiento > 0), 2)}\")\n",
    "    print(f\"Neutros  : {round(100 * np.mean(df.sentimiento == 0), 2)}\")\n",
    "    print(f\"Negativos: {round(100 * np.mean(df.sentimiento < 0), 2)}\")\n",
    "\n",
    "\n",
    "perfil_sentimientos(\"Train: \", score_train)\n",
    "print()\n",
    "perfil_sentimientos(\"Test: \", score_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Finalmente, agregaremos nuestro puntaje como columnas nuevas del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_train = pd.merge(ds_train, score_train, on='id')\n",
    "ds_train.rename(columns={'sentimiento': 'score_sentimientos'}, inplace=True)\n",
    "ds_test = pd.merge(ds_test, score_test, on='id')\n",
    "ds_test.rename(columns={'sentimiento': 'score_sentimientos'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Tecnica Regex"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Revisamos las siguientes paginas para entender cuales son los ammenities mas buscados en CABA, y en Argentina en general.\n",
    "\n",
    "https://www.iprofesional.com/negocios/371702-cuales-son-los-amenities-mas-exoticos-de-edificios-en-argentina\n",
    "\n",
    "https://www.baenegocios.com/sociedad/Ranking-de-amenities-los-servicios-que-mas-pesan-al-comprar-una-propiedad-20220119-0068.html\n",
    "\n",
    "https://www.forbesargentina.com/negocios/amenities-servicios-mas-demandados-argentinos-comprar-una-propiedad-n11901\n",
    "\n",
    "Dichos ammenities parecen hacer que la propiedad cotice entre un 15% y un 20% más que el precio de venta.\n",
    "\n",
    "Sacando un promedio y haciendo un top-5 ranking, podemos notar que los mas relevantes son:\n",
    "\n",
    "- Garage/Estacionamiento\n",
    "- Pileta\n",
    "- Jardin/Espacio al aire libre\n",
    "- Parrilla\n",
    "- SUM (Gimnasio/Spa/Sauna)\n",
    "\n",
    "Al buscar estos datos, podriamos tratar de entender si el precio resulta mayor, contra una propiedad de similares caracteristicas pero sin estos ammenities y a partir de eso, entender que % varía del precio de venta original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calculate_freq(feature, regex):\n",
    "    freq = descriptions_train.property_description.str.contains(regex, regex=True).sum()\n",
    "    print(\n",
    "        f\"Los anuncios de propiedades que tienen la feature {feature} son: {freq} y representan el {freq * 100 // len(descriptions_train)}% de los datos\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Amenities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "garage = re.compile(r\"\\s*garage|garaje|estacionamiento|parking\")\n",
    "calculate_freq(\"Garage\", garage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "pileta = re.compile(r\"\\s*pileta\")\n",
    "calculate_freq(\"Pileta\", pileta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "jardin = re.compile(r\"\\s*jardin|espacio verde\")\n",
    "calculate_freq(\"Jardín\", jardin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "parrilla = re.compile(r\"\\s*parrilla|bbq\")\n",
    "calculate_freq(\"Parrilla\", parrilla)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sum = re.compile(r\"\\s*zoom|sum|gimansio|spa\")\n",
    "calculate_freq(\"SUM\", sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "balcon = re.compile(r\"\\s*balcon|balcón\")\n",
    "calculate_freq(\"Balcón\", balcon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Otro aspecto interesante que dejamos fuera del análisis es a que tipo de vivienda pertenece cada ammenity. Y si donde encontramos una amenity en particular, encontramos consecuentemente otra. Por ejemplo, una casa con jardin y parrila y/o pileta. De esta manera podriamos tratar de determinar el costo de cada ammenity o como afecta al precio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Por último, construiremos columnas booleanas para los mejores features y las agregaremos a nuestros datasets. Usaremos parrilla, sum, balcón y pileta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "amenities_train = pd.DataFrame({\n",
    "    'id': descriptions_train.id,\n",
    "    'pileta': descriptions_train.property_description.str.contains(pileta, regex=True),\n",
    "    'parrilla': descriptions_train.property_description.str.contains(parrilla, regex=True),\n",
    "    'balcon': descriptions_train.property_description.str.contains(balcon, regex=True),\n",
    "    'sum': descriptions_train.property_description.str.contains(sum, regex=True)\n",
    "})\n",
    "amenities_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Replicamos lo mismo en test\n",
    "amenities_test = pd.DataFrame({\n",
    "    'id': descriptions_test.id,\n",
    "    'pileta': descriptions_test.property_description.str.contains(pileta, regex=True),\n",
    "    'parrilla': descriptions_test.property_description.str.contains(parrilla, regex=True),\n",
    "    'balcon': descriptions_test.property_description.str.contains(balcon, regex=True),\n",
    "    'sum': descriptions_test.property_description.str.contains(sum, regex=True)\n",
    "})\n",
    "amenities_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_train = pd.merge(ds_train, amenities_train, on='id')\n",
    "ds_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_test = pd.merge(ds_test, amenities_test, on='id')\n",
    "ds_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Expensas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Por último, trabajaremos en crear una columna numérica con el valor de las expensas. Evaluaremos primero que porcentaje de valores podemos conseguir con regex"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "expensas = re.compile(r\"\\s*[0-9.]*\\s*exp|expensas\")\n",
    "calculate_freq(\"Expensas\", expensas)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "expensas_extract = re.compile('((?:[a-zA-Z0-9]+\\s*){5}(?:expensas|exp)\\s(?:[a-zA-Z0-9]+\\s){10})')\n",
    "expensas_train = descriptions_train.property_description.str.extract(expensas_extract)\n",
    "expensas_train.value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "Input \u001B[0;32mIn [43]\u001B[0m, in \u001B[0;36m<cell line: 2>\u001B[0;34m()\u001B[0m\n\u001B[1;32m      1\u001B[0m expensas_extract \u001B[38;5;241m=\u001B[39m re\u001B[38;5;241m.\u001B[39mcompile(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m((?:[a-zA-Z0-9]+\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms*)\u001B[39m\u001B[38;5;132;01m{5}\u001B[39;00m\u001B[38;5;124m(?:expensas|exp)\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms(?:[a-zA-Z0-9]+\u001B[39m\u001B[38;5;124m\\\u001B[39m\u001B[38;5;124ms)\u001B[39m\u001B[38;5;132;01m{10}\u001B[39;00m\u001B[38;5;124m)\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m----> 2\u001B[0m expensas_train \u001B[38;5;241m=\u001B[39m \u001B[43mdescriptions_train\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mproperty_description\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mstr\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mextract\u001B[49m\u001B[43m(\u001B[49m\u001B[43mexpensas_extract\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m      3\u001B[0m expensas_train\u001B[38;5;241m.\u001B[39mvalue_counts()\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/strings/accessor.py:125\u001B[0m, in \u001B[0;36mforbid_nonstring_types.<locals>._forbid_nonstring_types.<locals>.wrapper\u001B[0;34m(self, *args, **kwargs)\u001B[0m\n\u001B[1;32m    120\u001B[0m     msg \u001B[38;5;241m=\u001B[39m (\n\u001B[1;32m    121\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCannot use .str.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfunc_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m with values of \u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    122\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124minferred dtype \u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;132;01m{\u001B[39;00m\u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_inferred_dtype\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m.\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[1;32m    123\u001B[0m     )\n\u001B[1;32m    124\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mTypeError\u001B[39;00m(msg)\n\u001B[0;32m--> 125\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mfunc\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[38;5;241;43m*\u001B[39;49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/strings/accessor.py:2547\u001B[0m, in \u001B[0;36mStringMethods.extract\u001B[0;34m(self, pat, flags, expand)\u001B[0m\n\u001B[1;32m   2544\u001B[0m     result \u001B[38;5;241m=\u001B[39m DataFrame(columns\u001B[38;5;241m=\u001B[39mcolumns, dtype\u001B[38;5;241m=\u001B[39mresult_dtype)\n\u001B[1;32m   2546\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m-> 2547\u001B[0m     result_list \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;43mself\u001B[39;49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_data\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43marray\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43m_str_extract\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2548\u001B[0m \u001B[43m        \u001B[49m\u001B[43mpat\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mflags\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mflags\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mexpand\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mreturns_df\u001B[49m\n\u001B[1;32m   2549\u001B[0m \u001B[43m    \u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2551\u001B[0m     result_index: Index \u001B[38;5;241m|\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[1;32m   2552\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(obj, ABCSeries):\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/strings/object_array.py:483\u001B[0m, in \u001B[0;36mObjectStringArrayMixin._str_extract\u001B[0;34m(self, pat, flags, expand)\u001B[0m\n\u001B[1;32m    480\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    481\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m empty_row\n\u001B[0;32m--> 483\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [f(val) \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39masarray(\u001B[38;5;28mself\u001B[39m)]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/strings/object_array.py:483\u001B[0m, in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m    480\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m    481\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m empty_row\n\u001B[0;32m--> 483\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m [\u001B[43mf\u001B[49m\u001B[43m(\u001B[49m\u001B[43mval\u001B[49m\u001B[43m)\u001B[49m \u001B[38;5;28;01mfor\u001B[39;00m val \u001B[38;5;129;01min\u001B[39;00m np\u001B[38;5;241m.\u001B[39masarray(\u001B[38;5;28mself\u001B[39m)]\n",
      "File \u001B[0;32m~/anaconda3/lib/python3.9/site-packages/pandas/core/strings/object_array.py:477\u001B[0m, in \u001B[0;36mObjectStringArrayMixin._str_extract.<locals>.f\u001B[0;34m(x)\u001B[0m\n\u001B[1;32m    475\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(x, \u001B[38;5;28mstr\u001B[39m):\n\u001B[1;32m    476\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m empty_row\n\u001B[0;32m--> 477\u001B[0m m \u001B[38;5;241m=\u001B[39m \u001B[43mregex\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43msearch\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    478\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m m:\n\u001B[1;32m    479\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m [na_value \u001B[38;5;28;01mif\u001B[39;00m item \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m \u001B[38;5;28;01melse\u001B[39;00m item \u001B[38;5;28;01mfor\u001B[39;00m item \u001B[38;5;129;01min\u001B[39;00m m\u001B[38;5;241m.\u001B[39mgroups()]\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 43
  },
  {
   "cell_type": "markdown",
   "source": [
    "expensas_extract = re.compile('((?:[0-9a-zA-Z,.]+\\s*){5}?(?:con|sin|de)\\s*(?:expensas|exp))')\n",
    "expensas_train = descriptions_train.sample(100).property_description.str.extract(expensas_extract)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "sin_expensas = re.compile('\\s*([0-9.]+)\\s*exp|expensas')\n",
    "descriptions_train.property_description.str.extract(sin_expensas)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Selección de features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Para entrenar los modelos usaremos nuestro dataset recien generado, descartaremos el id, el título y las fechas. Convertiremos las categóricas en variables numéricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_trabajo_train = ds_train.drop(['id', 'property_title', 'start_date', 'end_date'], axis=1)\n",
    "ds_trabajo_train['place_l3'] = pd.factorize(ds_train['place_l3'])[0]\n",
    "ds_trabajo_train['property_type'] = pd.factorize(ds_train['property_type'])[0]\n",
    "ds_trabajo_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Replicamos en test\n",
    "ds_trabajo_test = ds_test.drop(['id', 'property_title', 'start_date', 'end_date'], axis=1)\n",
    "ds_trabajo_test['place_l3'] = pd.factorize(ds_test['place_l3'])[0]\n",
    "ds_trabajo_test['property_type'] = pd.factorize(ds_test['property_type'])[0]\n",
    "ds_trabajo_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ds_trabajo_train.shape, ds_trabajo_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Sacamos la variable target y creamos nuestros datasets de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "columnas_predictoras = ds_trabajo_train.columns.to_list()\n",
    "columnas_predictoras.remove('property_price')\n",
    "columnas_predictoras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train_tp1 = ds_trabajo_train.loc[:, ['latitud',\n",
    "                                       'longitud',\n",
    "                                       'place_l3',\n",
    "                                       'property_type',\n",
    "                                       'property_rooms',\n",
    "                                       'property_bedrooms',\n",
    "                                       'property_surface_total',\n",
    "                                       'property_surface_covered']]\n",
    "x_train = ds_trabajo_train.loc[:, columnas_predictoras]\n",
    "x_test = ds_trabajo_test.loc[:, columnas_predictoras]\n",
    "\n",
    "y_train = ds_trabajo_train.property_price\n",
    "y_test = ds_trabajo_test.property_price"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Exportamos los dataset de train y test:"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pd.concat([x_train, y_train], axis=1).to_csv('datasets/train.csv')\n",
    "pd.concat([x_test, y_test], axis=1).to_csv('datasets/test.csv')"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x_train"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Como todas las features que tenemos están en escalas completamente diferentes y no pueden compararse, normalizaremos el dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sscaler = StandardScaler()\n",
    "x_train_tp1_transform = sscaler.fit_transform(pd.DataFrame(x_train_tp1))\n",
    "x_train_transform = sscaler.fit_transform(pd.DataFrame(x_train))\n",
    "x_test_transform = sscaler.fit_transform(pd.DataFrame(x_test))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XGBoost - Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "best_xgb_tp1 es el arbol que tiene los mejores hiper parametros y estimaodores obtenidos en el TP1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def regression_metrics(title, real, predicted):\n",
    "    mse = sk.metrics.mean_squared_error(y_true=real, y_pred=predicted)\n",
    "    rmse = sk.metrics.mean_squared_error(y_true=real, y_pred=predicted, squared=False)\n",
    "    r2 = sk.metrics.r2_score(y_true=real, y_pred=predicted)\n",
    "\n",
    "    print(title)\n",
    "    print(f\"El error (mse) es: {mse}\")\n",
    "    print(f\"El error (rmse) es: {rmse}\")\n",
    "    print(f\"El error (r²) es: {r2}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_tp1 = load('XGBoost-pca.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb_tp1.fit(x_train_transform, y_train)\n",
    "y_pred = best_xgb_tp1.predict(x_train_transform)\n",
    "y_pred_test = best_xgb_tp1.predict(x_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Metricas obtenidas con el dataset del TP1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metricas XGBoost optimizado - Train\n",
    "\n",
    "El error (mse) es: 761704416.3937123\n",
    "\n",
    "El error (rmse) es: 27598.993032241455\n",
    "\n",
    "El error (r²) es: 0.9652453545965177\n",
    "\n",
    "---------------------------------------\n",
    "\n",
    "Metricas XGBoost optimizado - Test\n",
    "\n",
    "El error (mse) es: 3287902738.8784523\n",
    "\n",
    "El error (rmse) es: 57340.23664825994\n",
    "\n",
    "El error (r²) es: 0.8477123168506954"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_metrics(\"Metricas XGBoost optimizado - Train\", y_train, y_pred)\n",
    "regression_metrics(\"Metricas XGBoost optimizado - Test\", y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos esta pagina como soporte para entender mejor que representa cada metrica de error: \n",
    "\n",
    "https://sitiobigdata.com/2018/08/27/machine-learning-metricas-regresion-mse/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**\n",
    "\n",
    "Notamos como el modelo con el dataset expandido mejora significativamente.\n",
    "\n",
    "Podemos observar un delta de **MSE** de: 432564299. Esto significa que el error se redujo un 400%. Esto es relevante ya que esta metrica nos dice sobre cuan bueno es realmente el modelo entrenado.\n",
    "\n",
    "Luego, para **RMSE** tenemos un delta de: 9456. Esto significa que el error se redujo un 65%.\n",
    "\n",
    "Finalmente para **R2** obtuvimos un delta de: -0,19. Esto significa que el modelo mejoró un 20%, ya que, mientras mas tengamos un valor mas cercano a uno, tenemos un modelo con un error cercano a cero."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**\n",
    "\n",
    "Notamos como el modelo con el dataset expandido mejora significativamente.\n",
    "\n",
    "Podemos observar un delta de **MSE** de: -96136560337. Esto significa que el error en test empeoró un 3%. \n",
    "\n",
    "Luego, para **RMSE** tenemos un delta de: -257976,21245748526. Esto significa que el error empeoró un 550%.\n",
    "\n",
    "Finalmente para **R2** obtuvimos un delta de: 0,52. Esto significa que el modelo empeoró un 52%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Nuevos Hiperparametros optimizados con el nuevo dataset ampliado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "\n",
    "#Cantidad de combinaciones que quiero porbar\n",
    "n = 10\n",
    "\n",
    "params = {\n",
    "    \"max_depth\": [3, 12, 4],\n",
    "    \"learning_rate\": [0.02, 0.03, 0.06],\n",
    "    \"min_child_weight\": [2, 12, 2],\n",
    "    \"n_estimators\": [100, 350],\n",
    "    'alpha': np.linspace(0.03, 0.09, n),\n",
    "}\n",
    "\n",
    "kfold = KFold(n_splits=5)\n",
    "\n",
    "search_regressor = XGBRegressor()\n",
    "\n",
    "search = RandomizedSearchCV(search_regressor, params, cv=5, random_state=9, n_iter=10, verbose=10000)\n",
    "\n",
    "search.fit(x_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejores Hiperparámetros\n",
    "search.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mejor Metrica\n",
    "search.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_xgb = search.best_estimator_\n",
    "best_xgb.fit(x_train_transform, y_train)\n",
    "y_pred = best_xgb.predict(x_train_transform)\n",
    "y_pred_test = best_xgb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_metrics(\"Metricas XGBoost optimizado - Train\", y_train, y_pred)\n",
    "regression_metrics(\"Metricas XGBoost optimizado - Test\", y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Train**\n",
    "\n",
    "Notamos como el modelo con el dataset sigue teniendo muy buenos resultados para el dataset de Train pero con una leve baja.\n",
    "\n",
    "Con respecto a las metricas obtenidas con el dataset del TP1, podemos observar un delta de **MSE** de: 617388041. Esto significa que el error, en comparacion con las metricas anteriores se incrementó un 287%.\n",
    "\n",
    "Luego, para **RMSE** tenemos un delta de: 12623. Esto significa que el error se redujo un 3%.\n",
    "\n",
    "Finalmente para **R2** obtuvimos un delta de: -0,02. Esto significa que el modelo empeoró un 2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test**\n",
    "\n",
    "Notamos como el modelo con el dataset expandido y la busqueda de un nuevo arbol con mejores hiper-parametros, resulto en un pequeño detrimento del dataset de train pero mejoró mucho mas el dataset de test.\n",
    "\n",
    "Con respecto a las metricas obtenidas con el dataset del TP1, podemos observar un delta de **MSE** de: 45032742345. Esto significa que el error en test mejoró un 54%. \n",
    "\n",
    "Luego, para **RMSE** tenemos un delta de: 82096. Esto significa que el error mejoró un 73%.\n",
    "\n",
    "Finalmente para **R2** obtuvimos un delta de: -9795260413860356. Esto significa que el modelo mejoró un 405%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------------------------------------------\n",
    "\n",
    "Como **conclusion**, notamos que el dataset expandido nos mejora el modelo con el dataset de train un 20% pero para el dataset de test, nos lo empeora un 52%.\n",
    "\n",
    "Cuando hicimos la busqueda de mejores hiper-parametros para el nuevo dataset expandido, logramos balancear estos resultados.\n",
    "\n",
    "Finalmente, haciendo el delta final, el resultado de expandir el dataset nos resulto en una mejora del 18% para el dataset de train y en un 353% de mejora en el dataset de test."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "### Redes Neuronales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Regresión"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Usaremos el dataset del tp1 normalizado"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "x_train_tp1_transform"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Predecir el precio de la propiedad y utilizar como métrica de evaluación el error cuadrático medio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Vamos a predecir el precio de la propiedad (dolares) en base a la superifice total y cubierta. Ya que como vimos en el trabajo pasado, eran los atributos que mas se correlacionaban con el precio.\n",
    "\n",
    "(Todas columnas tienen que ser numericas para Redes Neuronales)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def regression_scatter(x, y_true, y_pred):\n",
    "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "    fig.suptitle(f\"Precio según {x.name}\")\n",
    "    sns.scatterplot(x=x, y=y_true, ax=ax1)\n",
    "    ax1.set_title(f\"{x.name} vs Precio real\")\n",
    "\n",
    "    sns.scatterplot(x=x, y=y_pred, ax=ax2)\n",
    "    ax1.set_title(f\"{x.name} vs Precio predicho\")\n",
    "\n",
    "    sns.scatterplot(x=x, y=y_true, ax=ax3)\n",
    "    sns.scatterplot(x=x, y=y_pred, ax=ax3)\n",
    "    ax3.set_title(f\"Grafico combinado\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def plot_regression(x, y_true, y_pred, title=\"\", xlabel=\"x\", ylabel=\"y\"):\n",
    "    plt.figure()\n",
    "\n",
    "    plt.plot(x, y_true, \"o\", label=\"Valores verdaderos\")\n",
    "    plt.plot(x, y_pred, \"x\", label=\"Valores estimados\")\n",
    "\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def calcular_metricas_error(title, real, predicted):\n",
    "    mae = mean_absolute_error(real, predicted)\n",
    "    mse = mean_squared_error(real, predicted)\n",
    "    rmse = mean_squared_error(real, predicted, squared=False)\n",
    "    r2 = r2_score(real, predicted)\n",
    "\n",
    "    print(title)\n",
    "    print(f\"Error absoluto medio {mae}\")\n",
    "    print(f\"Error cuadrático medio {mse}\")\n",
    "    print(f\"Raiz del error cuadrático medio {rmse}\")\n",
    "    print(f\"R² {r2}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Construcción del modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Modelo base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Entrenaremos este modelo base con una Cross validation de 10 folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "d_in = x_train_tp1_transform.shape[1]\n",
    "d_out = 1\n",
    "\n",
    "\n",
    "def base_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(13, input_shape=(d_in,), kernel_initializer='normal', activation='relu'),\n",
    "        keras.layers.Dense(d_out, kernel_initializer='normal', activation='relu')])\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(model=base_model, batch_size=5, epochs=100)\n",
    "kfold = KFold(n_splits=5)\n",
    "results_base = cross_val_score(estimator, x_train_tp1_transform, y_train, cv=kfold, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Baseline: %.2f (%.2f) MSE\" % (results_base.mean(), results_base.std()))\n",
    "print(\"La raíz del error cuadrático medio para el modelo base es: %.2f\" % np.sqrt(abs(results_base.mean())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Ahora probaremos lo mismo pero en un modelo un poco más grande, agregaremos una capa intermedia. Reduciremos los splits a 5 para reducir un poco el tiempo de ejecución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Modelo de 3 capas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def larger_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(d_in, input_shape=(d_in,), kernel_initializer='normal', activation='relu'),\n",
    "        keras.layers.Dense(int(d_in/2), kernel_initializer='normal', activation='relu'),\n",
    "        keras.layers.Dense(1, kernel_initializer='normal', activation='relu')\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=larger_model, batch_size=5, epochs=100)\n",
    "kfold = KFold(n_splits=5)\n",
    "results_larger = cross_val_score(estimator, x_train_tp1_transform, y_train, cv=kfold, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "print(\"El MSE %.2f (%.2f)\" % (results_larger.mean(), results_larger.std()))\n",
    "print(f\"La raíz del error cuadrático medio para el modelo profundo es {np.sqrt(abs(results_larger.mean()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Por último, probaremos un tercer modelo con una capa inical más ancha."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Modelo ancho"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def wider_model():\n",
    "    model = keras.Sequential([\n",
    "        keras.layers.Dense(d_in*2, input_shape=(d_in,), kernel_initializer='normal', activation='relu'),\n",
    "        keras.layers.Dense(1, kernel_initializer='normal', activation='relu')\n",
    "    ])\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "estimator = KerasRegressor(build_fn=wider_model, batch_size=5, epochs=100)\n",
    "kfold = KFold(n_splits=5)\n",
    "results_wider = cross_val_score(estimator, x_train_tp1_transform, y_train, cv=kfold, scoring='neg_mean_squared_error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"El MSE %.2f (%.2f)\" % (results_wider.mean(), results_wider.std()))\n",
    "print(f\"La raíz del error cuadrático medio para el modelo ancho es {np.sqrt(abs(results_wider.mean()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Análisis de métricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "print(\"La raíz del error cuadrático medio para el modelo base es: %.2f\" % np.sqrt(abs(results_base.mean())))\n",
    "print(\"La raíz del error cuadrático medio para el modelo profundo es: %.2f\" % np.sqrt(abs(results_larger.mean())))\n",
    "print(f\"La raíz del error cuadrático medio para el modelo ancho es {np.sqrt(abs(results_wider.mean()))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Analizaremos un poco más en detalle el modelo profundo que tiene mejor métrica."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model = larger_model()\n",
    "\n",
    "model.fit(x_train_transform, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred = model.predict(x_train_transform)\n",
    "y_pred_test = model.predict(x_test_transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "precios_train = pd.concat([y_train, pd.DataFrame(y_pred, columns=['predicted'])], axis=1)\n",
    "precios_test = pd.concat([y_test, pd.DataFrame(y_pred_test, columns=['predicted'])], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 5))\n",
    "fig.suptitle(\"Distribución de precios reales y predichos Train\")\n",
    "\n",
    "sns.kdeplot(precios_train['property_price'], ax=ax1)\n",
    "sns.kdeplot(y_pred[:, 0], ax=ax2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Podemos ver que la campana real es bastante similar a la predicha tanto en train como en test. La campana predicha es levemente más baja habiendo menos densidad en los precios medios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "calcular_metricas_error(\"Metricas de train:\", y_train, y_pred)\n",
    "calcular_metricas_error(\"Metricas de test:\", y_test, y_pred_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "regression_scatter(ds_train.property_surface_covered, y_train, y_pred[:, 0])\n",
    "regression_scatter(ds_train.property_surface_total, y_train, y_pred[:, 0])\n",
    "regression_scatter(ds_train.latitud, y_train, y_pred[:, 0])\n",
    "regression_scatter(ds_train.longitud, y_train, y_pred[:, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Clasificación"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Construcción del target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### DS Expandido:\n",
    "\n",
    "### top3 > % regex -> booleanas\n",
    "### expensas sacadas con regex --> numerico\n",
    "### property_desc_score = tecnica chino -> numerico\n",
    "### serian 5 columnas en total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Otro factor interesante que podriamos analizar es a que tipo de vivienda pertenece cada ammenity. Y si donde encontramos una ammenity en particular, encontramos consecuentemente otra. Por ejemplo, una casa con jardin y parrila y/o pileta.\n",
    "De esta manera podriamos tratar de determinar el costo de cada ammenity o como afecta al precio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Ensambles de modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ensamble 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Ensamble 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "tags": []
   },
   "source": [
    "## Conclusiones"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}